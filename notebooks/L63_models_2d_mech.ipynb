{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# local_path = \"/homes/s17ouala/Bureau/Sanssauvegarde/NbedDyn-main\"\n",
    "local_path = '/Users/matthewlevine/code_projects/NbedDyn'\n",
    "sys.path.append(os.path.join(local_path, 'code/modules'))\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from dynamical_models   import *\n",
    "from generate_data      import *\n",
    "from NbedDyn            import *\n",
    "from stat_functions     import *\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD:\n",
    "    model = 'Lorenz_63'\n",
    "    class parameters:\n",
    "        sigma = 10.0\n",
    "        rho = 28.0\n",
    "        beta = 8.0/3\n",
    "    dt_integration = 0.01 # integration time\n",
    "    dt_states = 1 # number of integeration times between consecutive states (for xt and catalog)\n",
    "    dt_obs = 8# number of integration times between consecutive observations (for yo)\n",
    "    var_obs = np.array([0,1]) # indices of the observed variables\n",
    "    nb_loop_train = 110.01 # size of the catalog\n",
    "    nb_loop_test = 100 # size of the true state and noisy observations\n",
    "    sigma2_catalog = 0.0 # variance of the model error to generate the catalog\n",
    "    sigma2_obs = 0.0 # variance of the observation error to generate observation\n",
    "\n",
    "# run the data generation\n",
    "X_train_chaos = Attractor_Lorenz = np.load(os.path.join(local_path,'data/X_train_L63_CHAOS.npy'))\n",
    "X_test  = np.load(os.path.join(local_path,'data/X_test_CHAOS.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_obs      = 2\n",
    "X_train    = X_train_chaos[:,:d_obs]#[:-1,:1]\n",
    "Grad_t     = np.gradient(X_train,axis=0)/GD.dt_integration\n",
    "Batch_size = X_train.shape[0]\n",
    "nb_batch   = int(X_train.shape[0]/Batch_size)\n",
    "X_train    = X_train.reshape(nb_batch,Batch_size,d_obs)\n",
    "Grad_t     = Grad_t.reshape(nb_batch,Batch_size,d_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train    = X_train_chaos[:,:1]#[:-1,:1]\n",
    "# Grad_t     = np.gradient(X_train[:,0]).reshape(X_train.shape[0],1)/GD.dt_integration\n",
    "# Batch_size = X_train.shape[0]\n",
    "# nb_batch   = int(X_train.shape[0]/Batch_size)\n",
    "# X_train    = X_train.reshape(nb_batch,Batch_size,1)\n",
    "# Grad_t     = Grad_t.reshape(nb_batch,Batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "lr = 0.001\n",
      "Training L63 NbedDyn model 0 tensor(4458.4014, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "N_lat = 2\n",
    "for seed in range(0,1):\n",
    "    params = {}\n",
    "    params['use_f0']             = True\n",
    "    params['seed']               = seed\n",
    "    params['transition_layers']  = 1\n",
    "    params['bi_linear_layers']   = N_lat+d_obs\n",
    "    params['dim_hidden_linear']  = N_lat+d_obs\n",
    "    params['dim_input']          = d_obs\n",
    "    params['dim_latent']         = N_lat\n",
    "    params['dim_observations']   = d_obs\n",
    "    params['dim_Embedding']      = N_lat+d_obs\n",
    "    params['ntrain']             = [30000,1000]\n",
    "    params['dt_integration']     = 0.01\n",
    "    params['pretrained']         = False\n",
    "    params['nb_batch']           = nb_batch\n",
    "    params['Batch_size']         = Batch_size\n",
    "    params['get_latent_train']   = False\n",
    "    params['path']               = ''\n",
    "    params['file_name']          = 'NbedDyn_L63_2d_dim_'+str(params['dim_Embedding'])+'_seed_'+str(params['seed'])\n",
    "    model, modelRINN = get_NbedDyn_model(params)\n",
    "    model, modelRINN, aug_inp_data = train_NbedDyn_model_L63(params,model,modelRINN,X_train,Grad_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [torch.cat((torch.from_numpy(X_train).float()[-1,-1:,:], modelRINN.Dyn_net.y_aug[-1,-1:,:]), dim=1)]\n",
    "for i in range(10000):\n",
    "    pred.append(modelRINN(pred[-1],params['dt_integration'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasting_err=np.zeros(1000)\n",
    "for i in range(1,100):\n",
    "    forecasting_err[i-1]=RMSE(X_test[i-1,:d_obs],torch.stack(pred).data.numpy()[i,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prediction error at t0 + dt : '  ,forecasting_err[0])\n",
    "print('prediction error at t0 + 4dt : ' ,forecasting_err[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.stack(pred)[:2000,0,:].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forecast at test time :\n",
    "1) viz test time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast at starting from t0 = 400*dt :\n",
    "plt.plot(X_test[400:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple method : find analogs for the latent states in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init(test_vars,length_prior,dt,dim_aug):\n",
    "    x = torch.from_numpy(X_train).float()\n",
    "    aug_inp = torch.cat((x[0,:,:], modelRINN.Dyn_net.y_aug[0,:,:]), dim=1)\n",
    "    pred, grad, inp, aug_inp = modelRINN(aug_inp,dt)\n",
    "    pred, aug = model_Multi_RINN_simple(aug_inp, 0.0, length_prior, dt)\n",
    "    loss_init=[]\n",
    "    for i in range(aug_inp.shape[0]):\n",
    "        loss_init.append(((pred[1:,i,:d_obs][torch.where(~torch.isnan(test_vars))]-test_vars[torch.where(~torch.isnan(test_vars))])**2).mean())\n",
    "    min_idx = np.where(torch.stack(loss_init).data.numpy()==torch.stack(loss_init).data.numpy().min())\n",
    "    inp_init = aug_inp.detach().data.numpy()[min_idx[0][0]].reshape(1,dim_aug)\n",
    "    inp_init = (torch.from_numpy(inp_init).float())\n",
    "    return inp_init\n",
    "class Multi_INT_net(torch.nn.Module):\n",
    "        def __init__(self, params):\n",
    "            super(Multi_INT_net, self).__init__()\n",
    "#            self.add_module('Dyn_net',FC_net(params))\n",
    "            self.Int_net = modelRINN\n",
    "        def forward(self, inp, t0, nb, dt):\n",
    "            \"\"\"\n",
    "            In the forward function we accept a Tensor of input data and we must return\n",
    "            a Tensor of output data. We can use Modules defined in the constructor as\n",
    "            well as arbitrary operators on Tensors.\n",
    "            \"\"\"\n",
    "#            dt = Variable(torch.from_numpy(np.reshape(dt,(1,1))).float())\n",
    "#            x = Variable(3*torch.ones(1, 1), requires_grad=True)\n",
    "\n",
    "            #grad, aug_inp = self.Dyn_net(inp,dt)\n",
    "            #pred = aug_inp +dt*grad\n",
    "            pred = [inp]\n",
    "            aug  = []\n",
    "            for i in range(nb):\n",
    "                predic, k1, inp, aug_inp = self.Int_net(pred[-1], dt)\n",
    "                pred.append(predic)\n",
    "                aug.append(aug_inp)\n",
    "            return torch.stack(pred), torch.stack(aug)\n",
    "model_Multi_RINN_simple = Multi_INT_net(params)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRINN.Dyn_net.y_aug.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cond_idx = 400\n",
    "length_prior     = 1 # length of the initial condition\n",
    "\n",
    "test_vars = (torch.from_numpy(np.reshape(X_test[initial_cond_idx:initial_cond_idx+length_prior,:d_obs],(length_prior,d_obs))).float())\n",
    "inp_init = get_init(test_vars,length_prior,0.01,params['dim_Embedding'])\n",
    "\n",
    "y_pred2=np.zeros((2000+length_prior+1,params['dim_Embedding']))\n",
    "tmp = inp_init\n",
    "y_pred2[0,:] = tmp.cpu().data.numpy()\n",
    "for k in range(1,2000+length_prior+1):\n",
    "    y_pred2[k,:] = modelRINN(tmp,0.01)[0].cpu().data.numpy()\n",
    "    tmp = (torch.from_numpy(np.reshape(y_pred2[k,:],(1,params['dim_Embedding']))).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred2[1:600+1,0])\n",
    "plt.plot(X_test[400:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred2[1:600+1,1])\n",
    "plt.plot(X_test[400:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cond_idx = 400\n",
    "length_prior     = 20\n",
    "\n",
    "test_vars = (torch.from_numpy(np.reshape(X_test[initial_cond_idx:initial_cond_idx+length_prior,0],(length_prior,1))).float())\n",
    "inp_init = get_init(test_vars,length_prior,0.01,params['dim_Embedding'])\n",
    "\n",
    "y_pred2=np.zeros((2000+length_prior+1,params['dim_Embedding']))\n",
    "tmp = inp_init\n",
    "y_pred2[0,:] = tmp.cpu().data.numpy()\n",
    "for k in range(1,2000+length_prior+1):\n",
    "    y_pred2[k,:] = modelRINN(tmp,0.01)[0].cpu().data.numpy()\n",
    "    tmp = (torch.from_numpy(np.reshape(y_pred2[k,:],(1,params['dim_Embedding']))).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred2[1:600+1,0])\n",
    "plt.plot(X_test[400:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred2[1:600+1,1])\n",
    "plt.plot(X_test[400:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second method : optimize initial condition of the latent states to forecast the initial condition of the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_condition(model, time_series, train_series, dt,lr_init, err_tol = 1E-4, n_train = 10000, get_init_in_train_set = True):\n",
    "    criterion = torch.nn.MSELoss()#reduction = 'sum')\n",
    "    if get_init_in_train_set :\n",
    "        print(time_series.shape)\n",
    "        inp_init = get_init(time_series,time_series.shape[0],0.01,params['dim_Embedding'])\n",
    "    else:\n",
    "        min_idx = None\n",
    "        inp_init = torch.rand(1,(train_series.shape[-1]+model.Int_net.Dyn_net.y_aug.shape[-1])).float()*0.0\n",
    "        inp_init[:,:time_series.shape[-1]] = time_series.clone()[:1,:]\n",
    "    init_cond_model = get_init_model(model,inp_init)\n",
    "    optimizer = torch.optim.Adam(init_cond_model.parameters(), lr = lr_init)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor = 0.1, patience=205, verbose=True, min_lr = 0.0001)\n",
    "    stop_cond = False\n",
    "    count = 0\n",
    "    while(stop_cond==False):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        pred = init_cond_model(0.0,time_series.shape[0],dt)\n",
    "        #pred1, grad, inp, aug_inp = modelRINN(test_vars[:1,:],dt, True, iterate = t)\n",
    "        # Compute and print loss\n",
    "        loss = torch.mean((pred[1:,0,:time_series.shape[-1]][torch.where(~torch.isnan(time_series))]- time_series[torch.where(~torch.isnan(time_series))])**2)\n",
    "#        criterion(pred[1:,0,:time_series.shape[-1]], time_series[:,:])\n",
    "        print(count,loss)\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        count += 1\n",
    "        if loss.detach().numpy()<err_tol or count>n_train:\n",
    "            stop_cond = True\n",
    "    return inp_init, init_cond_model.estimate_init\n",
    "\n",
    "class get_init_model(torch.nn.Module):\n",
    "        def __init__(self, model_Multi_RINN, inp_init):\n",
    "            super(get_init_model, self).__init__()\n",
    "            self.Multi_INT_net = model_Multi_RINN\n",
    "            self.estimate_init = torch.nn.Parameter((inp_init.clone()))#torch.nn.Parameter(aug_inp[:1,:])\n",
    "        def forward(self, t0, nb, dt):\n",
    "            pred = self.Multi_INT_net(self.estimate_init, t0, nb, dt)[0]\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cond_idx = 400\n",
    "length_prior     = 50\n",
    "train_series = torch.from_numpy(X_train).float()\n",
    "\n",
    "model_Multi_RINN = Multi_INT_net(params)\n",
    "for param in model_Multi_RINN.Int_net.parameters():\n",
    "   param.requires_grad = False \n",
    "\n",
    "inp_init_knn, inp_init_opti = get_initial_condition(model_Multi_RINN, torch.from_numpy(X_test[initial_cond_idx:initial_cond_idx+length_prior,:1]).float(), train_series, 0.01,lr_init = 0.1, err_tol = 1E-4, n_train = 10000, get_init_in_train_set = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=np.zeros((2000+length_prior+1,params['dim_Embedding']))\n",
    "tmp = inp_init_knn\n",
    "y_pred2[0,:] = tmp.cpu().data.numpy()\n",
    "for k in range(1,2000+length_prior+1):\n",
    "    y_pred2[k,:] = modelRINN(tmp,0.01)[0].cpu().data.numpy()\n",
    "    tmp = (torch.from_numpy(np.reshape(y_pred2[k,:],(1,params['dim_Embedding']))).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3=np.zeros((2000+length_prior+1,params['dim_Embedding']))\n",
    "tmp = inp_init_opti\n",
    "y_pred3[0,:] = tmp.cpu().data.numpy()\n",
    "for k in range(1,2000+length_prior+1):\n",
    "    y_pred3[k,:] = modelRINN(tmp,0.01)[0].cpu().data.numpy()\n",
    "    tmp = (torch.from_numpy(np.reshape(y_pred3[k,:],(1,params['dim_Embedding']))).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_pred2[1:600,0])\n",
    "plt.plot(X_test[400:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred3[1:600,0])\n",
    "plt.plot(X_test[400:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding an initial condition for noisy and missing observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.copy(X_test[:,:1])*np.nan\n",
    "observations[::8,:] = X_test[::8,:1] + np.random.normal(0,2,X_test[::8,:1].shape[0]).reshape(X_test[::8,:1].shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_cond_idx = 400\n",
    "length_prior     = 100\n",
    "train_series = torch.from_numpy(X_train).float()\n",
    "inp_init_knn, inp_init_opti = get_initial_condition(model_Multi_RINN, torch.from_numpy(observations[initial_cond_idx:initial_cond_idx+length_prior,:1]).float(), train_series, 0.01,lr_init = 0.02, err_tol = 1E-1, n_train = 500, get_init_in_train_set = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3=np.zeros((2000+length_prior+1,params['dim_Embedding']))\n",
    "tmp = inp_init_opti\n",
    "y_pred3[0,:] = tmp.cpu().data.numpy()\n",
    "for k in range(1,2000+length_prior+1):\n",
    "    y_pred3[k,:] = modelRINN(tmp,0.01)[0].cpu().data.numpy()\n",
    "    tmp = (torch.from_numpy(np.reshape(y_pred3[k,:],(1,params['dim_Embedding']))).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred3[1:600,0])\n",
    "plt.plot(X_test[400:,0])\n",
    "plt.plot(observations[400:,0],'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
